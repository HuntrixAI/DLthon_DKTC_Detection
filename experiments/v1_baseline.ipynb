{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {"provenance": [], "gpuType": "T4"},
  "kernelspec": {"name": "python3", "display_name": "Python 3"},
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ”¥ AIFFEL DLthon - DKTC í•œêµ­ì–´ ìœ„í˜‘ ëŒ€í™” ë¶„ë¥˜\n",
    "## Korean Dangerous Talk Classification\n",
    "\n",
    "**ì „ëµ: íƒ‘ë‹¤ìš´ ì ‘ê·¼ë²•**\n",
    "- Level 1: ë² ì´ìŠ¤ë¼ì¸ â†’ ë¬¸ì œ ë°œê²¬\n",
    "- Level 2: ìµœì‹  ë…¼ë¬¸ ê¸°ë²•ìœ¼ë¡œ ë¬¸ì œ í•´ê²°\n",
    "- Level 3: Ablation Studyë¡œ ì •ëŸ‰ ê²€ì¦\n",
    "\n",
    "**ì ìš© ê¸°ë²•:**\n",
    "| ê¸°ë²• | ë…¼ë¬¸ | ì—­í•  |\n",
    "|---|---|---|\n",
    "| KcELECTRA | Clark et al., 2020 | í•œêµ­ì–´ êµ¬ì–´ì²´ íŠ¹í™” ëª¨ë¸ |\n",
    "| Focal Loss | Lin et al., 2017 | í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° |\n",
    "| R-Drop | Liang et al., 2021 | ì†Œê·œëª¨ ë°ì´í„° ê³¼ì í•© ë°©ì§€ |\n",
    "| LLM í•©ì„± + ê³µê°œ ë°ì´í„° | AugGPT (Dai et al., 2023) ì˜ê° | ì¼ë°˜ëŒ€í™” í´ë˜ìŠ¤ êµ¬ì¶• |\n",
    "\n",
    "**í•™ìŠµ ì´ë ¥ í™œìš©:**\n",
    "- Ex03: Ablation Study ë°©ë²•ë¡ \n",
    "- Ex06: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, ê°ì„±ë¶„ì„ íŒŒì´í”„ë¼ì¸\n",
    "- Ex07: SentencePiece í† í¬ë‚˜ì´ì €\n",
    "- Ex09: Transformer êµ¬ì¡° ì´í•´ â†’ Fine-tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ“¦ STEP 0: í™˜ê²½ ì„¤ì •"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 0-1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ â•â•â•\n",
    "!pip install -q transformers datasets accelerate sentencepiece\n",
    "!pip install -q scikit-learn pandas matplotlib seaborn"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 0-2: ì™¸ë¶€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ â•â•â•\n",
    "# SmileStyle: í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” (17ê°œ ìŠ¤íƒ€ì¼)\n",
    "!wget -q https://raw.githubusercontent.com/smilegate-ai/korean_smile_style_dataset/main/smilestyle_dataset.tsv\n",
    "\n",
    "# KakaoChatData: ì‹¤ì œ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” 73Kê°œ\n",
    "!wget -q https://raw.githubusercontent.com/Ludobico/KakaoChatData/main/Dataset/ChatbotData.csv\n",
    "\n",
    "print('âœ… ì™¸ë¶€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ')\n",
    "!ls -la *.tsv *.csv 2>/dev/null || echo 'íŒŒì¼ í™•ì¸ í•„ìš”'"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 0-3: ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ & ì‹œë“œ ê³ ì • â•â•â•\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re, os, random, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# GPU í™•ì¸\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ğŸ–¥ï¸ Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œê°í™”ìš©)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 0-4: ê³µí†µ ìƒìˆ˜ & í´ë˜ìŠ¤ ë§¤í•‘ â•â•â•\n",
    "CLASS_MAP = {\n",
    "    'í˜‘ë°• ëŒ€í™”': 0,\n",
    "    'ê°ˆì·¨ ëŒ€í™”': 1,\n",
    "    'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”': 2,\n",
    "    'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”': 3,\n",
    "    'ì¼ë°˜ ëŒ€í™”': 4\n",
    "}\n",
    "CLASS_NAMES = list(CLASS_MAP.keys())\n",
    "NUM_CLASSES = 5\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print(f'í´ë˜ìŠ¤ ë§¤í•‘: {CLASS_MAP}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ“Š STEP 1: ë°ì´í„° ë¡œë“œ ë° EDA\n",
    "\n",
    "**[ë°°ìš´ ê²ƒ í™œìš©]** Ex06 ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„ì—ì„œ ë°°ìš´ ë°ì´í„° ë¶„ì„ ê¸°ë²• ì ìš©\n",
    "- í´ë˜ìŠ¤ ë¶„í¬, ë¬¸ì¥ ê¸¸ì´, ë¹ˆì¶œ ë‹¨ì–´ ë¶„ì„"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 1-1: ë°ì´í„° ë¡œë“œ â•â•â•\n",
    "# â€» Colabì— train.csv, test.csv, submission.csvë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "submission_df = pd.read_csv('submission.csv')\n",
    "\n",
    "print(f'Train: {train_df.shape}')\n",
    "print(f'Test:  {test_df.shape}')\n",
    "print(f'\\ní´ë˜ìŠ¤ ë¶„í¬:')\n",
    "print(train_df['class'].value_counts())\n",
    "print(f'\\nâš ï¸ ì¼ë°˜ ëŒ€í™”: 0ê°œ â†’ í•©ì„±ë°ì´í„° í•„ìˆ˜!')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 1-2: EDA ì‹œê°í™” â•â•â•\n",
    "train_df['conv_len'] = train_df['conversation'].str.len()\n",
    "train_df['label'] = train_df['class'].map(CLASS_MAP)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬\n",
    "train_df['class'].value_counts().plot(kind='bar', ax=axes[0],\n",
    "    color=['#e74c3c','#f39c12','#3498db','#2ecc71'])\n",
    "axes[0].set_title('Class Distribution')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ëŒ€í™” ê¸¸ì´ ë¶„í¬\n",
    "for cls in train_df['class'].unique():\n",
    "    subset = train_df[train_df['class']==cls]['conv_len']\n",
    "    axes[1].hist(subset, bins=30, alpha=0.5, label=cls)\n",
    "axes[1].set_title('Conversation Length by Class')\n",
    "axes[1].set_xlabel('Characters')\n",
    "axes[1].legend(fontsize=7)\n",
    "\n",
    "# ê¸¸ì´ boxplot\n",
    "train_df.boxplot(column='conv_len', by='class', ax=axes[2])\n",
    "axes[2].set_title('Length Boxplot')\n",
    "axes[2].set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nê¸¸ì´ í†µê³„:')\n",
    "print(train_df.groupby('class')['conv_len'].describe()[['mean','50%','max']])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 1-3: í´ë˜ìŠ¤ë³„ ë¹ˆì¶œ ë‹¨ì–´ (ìƒìœ„ 10ê°œ) â•â•â•\n",
    "for cls in train_df['class'].unique():\n",
    "    texts = ' '.join(train_df[train_df['class']==cls]['conversation'].tolist())\n",
    "    words = [w for w in texts.split() if len(w) > 1]\n",
    "    common = Counter(words).most_common(10)\n",
    "    print(f'\\n[{cls}]')\n",
    "    print(f\"  {', '.join([f'{w}({c})' for w, c in common])}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ­ STEP 2: ì¼ë°˜ ëŒ€í™” í•©ì„±ë°ì´í„° ìˆ˜ì§‘\n",
    "\n",
    "**[ìµœì‹  ê¸°ë²•]** AugGPT (Dai et al., 2023) ì˜ê° - LLM/ê³µê°œë°ì´í„°ë¡œ ì¼ë°˜ëŒ€í™” êµ¬ì¶•\n",
    "\n",
    "**[ê³¼ì œ ê·œì¹™]** ì¼ë°˜ëŒ€í™” í´ë˜ìŠ¤ëŠ” í•©ì„±ë°ì´í„°ë¡œ êµ¬ì„± (í•„ìˆ˜)\n",
    "\n",
    "### 5ê°œ ë°ì´í„° ì†ŒìŠ¤:\n",
    "| # | ì†ŒìŠ¤ | ì„¤ëª… | ëª©í‘œ ìˆ˜ |\n",
    "|---|---|---|---|\n",
    "| 1 | SmileStyle | í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” informal/chat ìŠ¤íƒ€ì¼ | 400ê°œ |\n",
    "| 2 | KakaoChatData | ì‹¤ì œ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” | 300ê°œ |\n",
    "| 3 | kor_unsmile clean | í˜ì˜¤ ì•„ë‹Œ ì¼ë°˜ êµ¬ì–´ì²´ | 200ê°œ |\n",
    "| 4 | NSMC ê¸ì • | ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ì¼ìƒ í‘œí˜„ | 100ê°œ |\n",
    "| 5 | ê²½ê³„ ì¼€ì´ìŠ¤ â­ | ìœ„í˜‘ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì¼ë°˜ | 25ê°œ |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 2-1: ì†ŒìŠ¤ë³„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ â•â•â•\n",
    "\n",
    "def load_smilestyle(path='smilestyle_dataset.tsv', max_n=400):\n",
    "    \"\"\"SmileStyleì—ì„œ informal/chat ìŠ¤íƒ€ì¼ ëŒ€í™” ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='\\t')\n",
    "        convs = []\n",
    "        for col in ['informal', 'chat', 'choding']:\n",
    "            if col in df.columns:\n",
    "                convs.extend(df[col].dropna().tolist())\n",
    "        convs = list(set(convs))\n",
    "        random.shuffle(convs)\n",
    "        result = [c for c in convs if 50 < len(str(c)) < 500][:max_n]\n",
    "        print(f'  SmileStyle: {len(result)}ê°œ')\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f'  SmileStyle ì‹¤íŒ¨: {e}')\n",
    "        return []\n",
    "\n",
    "def load_kakao(path='ChatbotData.csv', max_n=300):\n",
    "    \"\"\"ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë°ì´í„°ì—ì„œ ì¼ìƒëŒ€í™” ì¶”ì¶œ\"\"\"\n",
    "    threat_kw = ['ì£½','ë•Œë¦¬','ëˆ ë‚´','í˜‘ë°•','í­í–‰','ì‚´í•´','ì¹¼','ì´',\n",
    "                 'ê°€ë§Œ ì•ˆ','ì‹ ê³ ','ê²½ì°°','ê°•íƒˆ','ë‚©ì¹˜']\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if 'label' in df.columns:\n",
    "            df = df[df['label'] == 0]\n",
    "        q_col = 'Q' if 'Q' in df.columns else df.columns[0]\n",
    "        a_col = 'A' if 'A' in df.columns else df.columns[1]\n",
    "        convs = []\n",
    "        for _, row in df.iterrows():\n",
    "            txt = f\"{row[q_col]} {row[a_col]}\"\n",
    "            if not any(k in txt for k in threat_kw) and 20 < len(txt) < 500:\n",
    "                convs.append(txt)\n",
    "        random.shuffle(convs)\n",
    "        result = convs[:max_n]\n",
    "        print(f'  KakaoChat: {len(result)}ê°œ')\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f'  KakaoChat ì‹¤íŒ¨: {e}')\n",
    "        return []\n",
    "\n",
    "def load_unsmile(max_n=200):\n",
    "    \"\"\"kor_unsmileì—ì„œ clean(ë¹„í˜ì˜¤) ë¬¸ì¥ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        ds = load_dataset('smilegate-ai/kor_unsmile', split='train')\n",
    "        df = ds.to_pandas()\n",
    "        texts = df[df['clean']==1.0]['ë¬¸ì¥'].tolist()\n",
    "        filtered = [t for t in texts if 20 < len(str(t)) < 500]\n",
    "        random.shuffle(filtered)\n",
    "        result = filtered[:max_n]\n",
    "        print(f'  kor_unsmile clean: {len(result)}ê°œ')\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f'  kor_unsmile ì‹¤íŒ¨: {e}')\n",
    "        return []\n",
    "\n",
    "def load_nsmc(max_n=100):\n",
    "    \"\"\"NSMCì—ì„œ ê¸ì •ì  ì¼ìƒ í‘œí˜„ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        ds = load_dataset('nsmc', split='train')\n",
    "        df = ds.to_pandas()\n",
    "        pos = df[df['label']==1]['document'].dropna().tolist()\n",
    "        daily_kw = ['ì¬ë°Œ','ì¢‹ì•„','ì¶”ì²œ','ê°ë™','ìµœê³ ','ì›ƒê¸°',\n",
    "                    'í–‰ë³µ','ì‚¬ë‘','ê¸°ë¶„','ì¦ê±°','í¸í•˜']\n",
    "        filtered = [t for t in pos\n",
    "                    if any(k in str(t) for k in daily_kw)\n",
    "                    and 20 < len(str(t)) < 300]\n",
    "        random.shuffle(filtered)\n",
    "        result = filtered[:max_n]\n",
    "        print(f'  NSMC positive: {len(result)}ê°œ')\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f'  NSMC ì‹¤íŒ¨: {e}')\n",
    "        return []\n",
    "\n",
    "print('âœ… ìˆ˜ì§‘ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 2-2: ê²½ê³„ ì¼€ì´ìŠ¤ ì§ì ‘ ìƒì„± â­ â•â•â•\n",
    "# ìœ„í˜‘ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì¼ë°˜ëŒ€í™”ì¸ ê²½ìš°\n",
    "# â†’ ëª¨ë¸ì´ ì§„ì§œ ìœ„í˜‘ vs ë†ë‹´ì„ êµ¬ë¶„í•˜ëŠ” í•µì‹¬ ë°ì´í„°\n",
    "\n",
    "BOUNDARY_CASES = [\n",
    "    \"ì•¼ ì£½ì„ë˜ ã…‹ã…‹ ì•„ ì§„ì§œ ì›ƒê²¨ì„œ ì£½ê² ë‹¤ ì–´ì œ ê·¸ê±° ë´¤ì–´ ë­”ë° ìœ íŠœë¸Œì— ê³ ì–‘ì´ ì˜ìƒ ë³´ê³  ì§„ì§œ ë¹µ í„°ì¡Œì–´ ë‚˜ë„ ë³´ì—¬ì¤˜ ë³´ë‚´ì¤„ê²Œ\",\n",
    "    \"ëˆ ë‚´ë†” ã…‹ã…‹ ë°¥ê°’ ë„¤ê°€ ìœë‹¤ë©° ì§€ë‚œë²ˆì— ë‚´ê°€ ëƒˆì–ì•„ ë§ë‹¤ ê·¸ëŸ¼ ì˜¤ëŠ˜ì€ ë‚´ê°€ ë‚¼ê²Œ ê·¸ë˜ ë§›ìˆëŠ” ê±° ë¨¹ì ì‚¼ê²¹ì‚´ ì–´ë•Œ ì¢‹ì•„\",\n",
    "    \"ì•¼ ë„ˆ ì§„ì§œ ë•Œë¦¬ê³  ì‹¶ë‹¤ ì™œ ë§¨ë‚  ëŠ¦ì–´ ë¯¸ì•ˆ ë²„ìŠ¤ ë†“ì³¤ì–´ ë‹¤ìŒë¶€í„° ì¼ì° ë‚˜ì™€ ì•Œê² ì–´ ì§„ì§œ ë‹¤ìŒì— ë˜ ëŠ¦ìœ¼ë©´ ë°¥ ì‚¬ì•¼ ë¼\",\n",
    "    \"ì•„ ë¯¸ì³ ì§„ì§œ ì£½ê² ë‹¤ ì‹œí—˜ ë§í–ˆì–´ ëª‡ ì ì¸ë° ë§í•˜ê¸° ì‹«ì–´ ê´œì°®ì•„ ë‹¤ìŒì— ì˜í•˜ë©´ ë˜ì§€ ìœ„ë¡œí•´ì¤˜ì„œ ê³ ë§ˆì›Œ ì»¤í”¼ë‚˜ ë§ˆì‹œëŸ¬ ê°€ì\",\n",
    "    \"ì•¼ ë‚´ ëˆ ë‚´ë†” ë­ ì•„ê¹Œ ë‚´ê°€ ë¹Œë ¤ì¤€ ì²œì› ì•„ ë§ë‹¤ ë¯¸ì•ˆ ìŠì–´ë²„ë ¸ì–´ ê´œì°®ì•„ ë‹¤ìŒì— ì‚¬íƒ• í•˜ë‚˜ ì‚¬ì¤˜ ã…‹ã…‹ ì•Œê² ì–´\",\n",
    "    \"ë„ˆ ì˜¤ëŠ˜ ì£½ì—ˆì–´ ì™œ ì˜¤ëŠ˜ ê²Œì„ì—ì„œ ë‚´ê°€ ì´ê¸¸ ê±°ê±°ë“  ê¿ˆê¿” ã…‹ã…‹ ë‚´ê°€ ë” ì˜í•´ í•œíŒ í•˜ì ì½œ ì§€ëŠ” ì‚¬ëŒ ì•„ì´ìŠ¤í¬ë¦¼ ì‚¬ê¸°ë‹¤\",\n",
    "    \"ì§„ì§œ í­ë°œí•  ê²ƒ ê°™ì•„ ì™œ ë¬´ìŠ¨ ì¼ì´ì•¼ ê³¼ì œ ì–‘ì´ ë„ˆë¬´ ë§ì•„ì„œ ë¯¸ì³ë²„ë¦¬ê² ì–´ ê°™ì´ í•˜ì ê³ ë§ˆì›Œ ì¹´í˜ì—ì„œ í•˜ì ì¢‹ì•„ ê°€ì\",\n",
    "    \"ì•¼ ê°œë¹¡ì³ ì™œ ë¬´ìŠ¨ ì¼ì´ì•¼ í° ì•¡ì • ê¹¨ì¡Œì–´ ì•„ì´ê³  AS ê°€ë´ ë¹„ì‹¸ê² ë‹¤ ë³´í—˜ ì•ˆ ë“¤ì—ˆì–´ ì•„ ê·¸ê±´ ì¢€ ì•„ê¹ë‹¤ í° ë¹Œë ¤ì¤„ê¹Œ\",\n",
    "    \"ë„ˆ ë•Œë¬¸ì— í”¼ê³¤í•´ì„œ ì£½ê² ì–´ ì™œ ì–´ì œ ì „í™” 3ì‹œê°„ í–ˆì–ì•„ ã…‹ã…‹ ì¬ë°Œì—ˆëŠ”ë° ë‚˜ë„ ê·¸ë˜ë„ ì˜¤ëŠ˜ì€ ì¼ì° ìì ì•Œê² ì–´ êµ¿ë‚˜ì‡\",\n",
    "    \"ì•„ ì—´ë°›ì•„ ì£½ê² ë„¤ ì™œ ë²„ìŠ¤ ì½”ì•ì—ì„œ ë†“ì³¤ì–´ ã…‹ã…‹ ë‹¤ìŒ ê±° íƒ€ë©´ ë˜ì§€ 10ë¶„ í›„ì— ì˜¨ëŒ€ ê¸°ë‹¤ë¦¬ì í¸ì˜ì  ê°”ë‹¤ì˜¬ê²Œ ì‚¼ê°ê¹€ë°¥ ì‚¬ì™€\",\n",
    "    \"ì•¼ ë„ˆ ë‚˜í•œí…Œ ë§ì„ë˜ ë­ ë‚´ê°€ ë¼ë©´ ë¨¹ëŠ”ë° ì•ˆ ë¶ˆëŸ¬ì¤¬ì–ì•„ ë¯¸ì•ˆ ë‹¤ìŒì— ê°™ì´ ë¨¹ì ì§„ì§œì§€ ì•½ì†ì´ì•¼ ê¼­ ë¶ˆëŸ¬ì¤˜\",\n",
    "    \"ì•¼ ë„ˆ ë¯¸ì³¤ì–´ ì™œ ì´ ì¼€ì´í¬ ë§›ì´ ë¯¸ì³¤ì–´ ì§„ì§œ ì–´ë””ì„œ ìƒ€ì–´ ì—­ ì•ì— ìƒˆë¡œ ìƒê¸´ ë² ì´ì»¤ë¦¬ ë‚˜ë„ ê°€ë³´ê³  ì‹¶ë‹¤ ë‚´ì¼ ê°™ì´ ê°€ì\",\n",
    "    \"ë•Œë ¤ì¹˜ìš°ê³  ì‹¶ë‹¤ ë­˜ íšŒì‚¬ ì˜¤ëŠ˜ ì§„ì§œ í˜ë“¤ì—ˆì–´ ë¬´ìŠ¨ ì¼ì´ì•¼ ì•¼ê·¼ì´ì—ˆì–´ ê³ ìƒí–ˆë‹¤ ë§¥ì£¼ í•œì”í•˜ì ì¢‹ì•„ ì¹˜í‚¨ë„ ì‹œí‚¤ì\",\n",
    "    \"ì‚´ë ¤ì¤˜ ì™œ ë¬´ìŠ¨ ì¼ì´ì•¼ ê³¼ì œ ë§ˆê°ì´ ë‚´ì¼ì¸ë° ì•„ì§ ì‹œì‘ë„ ì•ˆ í–ˆì–´ í°ì¼ì´ë‹¤ ê°™ì´ í•˜ì ë„ì„œê´€ ê°ˆë˜ ì‘ ë¹¨ë¦¬ ê°€ì\",\n",
    "    \"ì£½ì—¬ì¤€ë‹¤ ì§„ì§œ ë­ ì´ ë¼ë©´ ë§›ì´ ì£½ì—¬ì¤˜ ì–´ë””ì„œ ìƒ€ì–´ í¸ì˜ì  ì‹ ìƒì´ì•¼ ë‚˜ë„ ì‚¬ë¨¹ì–´ë´ì•¼ê² ë‹¤ ê°•ì¶”ì•¼ ì €ë…ì— ë¨¹ì–´ë³¼ê²Œ\",\n",
    "    \"ë„ˆ ë‚´ì¼ ê°ì˜¤í•´ ì™œ ë¬´ìŠ¨ ì¼ì´ì•¼ ë‚´ì¼ ë³¼ë§ ëŒ€íšŒì–ì•„ ë‚˜í•œí…Œ ì§ˆ ê°ì˜¤í•˜ë¼ê³  ã…‹ã…‹ ê¿ˆê¿” ë‚´ê°€ ì´ê¸¸ ê±´ë° ë‚´ê¸°í•˜ì\",\n",
    "    \"í­íƒ„ ë§ì€ ê¸°ë¶„ì´ì•¼ ë­ ë¨¸ë¦¬ë¥¼ ì˜ëëŠ”ë° ë„ˆë¬´ ì§§ì•„ ã…‹ã…‹ ì‚¬ì§„ ë³´ì—¬ì¤˜ ì™€ ì§„ì§œ ì§§ë‹¤ ê¸ˆë°© ìë¼ ê·¸ë˜ë„ ì¢€ ì¶©ê²©ì´ì•¼\",\n",
    "    \"ì•¼ ê±°ê¸° ì„œ ì™œ ë’¤ì—ì„œ ë¶ˆëŸ¬ì„œ ë†€ëì–ì•„ ë¯¸ì•ˆ ã…‹ã…‹ ê°™ì´ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°€ì ë­ ë¨¹ì„ê¹Œ ëˆê¹ŒìŠ¤ ì–´ë•Œ ì¢‹ì•„ ë¹¨ë¦¬ ê°€ì\",\n",
    "    \"í˜‘ë°• ì•„ë‹Œë° ë‚´ì¼ ì•ˆ ì˜¤ë©´ ì§„ì§œ ì„œìš´í•´ ë­”ë° ìƒì¼íŒŒí‹° í•˜ë ¤ê³  í•˜ëŠ”ë° ì™€ì•¼ì§€ ëª°ëì–´ ë‹¹ì—°íˆ ê°ˆê²Œ ì„ ë¬¼ ì¤€ë¹„í• ê²Œ ê³ ë§ˆì›Œ\",\n",
    "    \"ë„ˆ ì˜¤ëŠ˜ í°ì¼ë‚œë‹¤ ì™œ ë¬´ìŠ¨ ì¼ì´ì•¼ ì¶•êµ¬ ì‹œí•©ì¸ë° ìƒëŒ€íŒ€ì´ ê°•í•´ ì´ê¸¸ ìˆ˜ ìˆì„ê¹Œ ì—°ìŠµ ì—´ì‹¬íˆ í–ˆì–ì•„ í™”ì´íŒ… ê³ ë§ˆì›Œ\",\n",
    "    \"ì¹¼ ê°™ì€ ì‚¬ëŒì´ì•¼ ëˆ„ê°€ ìš°ë¦¬ íŒ€ì¥ë‹˜ ì‹œê°„ ì•½ì† ì¹¼ ê°™ì•„ ì¢‹ì€ ê±° ì•„ë‹ˆì•¼ ë§ì•„ ë°°ìš¸ ì ì´ì•¼ ë‚˜ë„ ê·¸ëŸ¬ê³  ì‹¶ë‹¤\",\n",
    "    \"ì•¼ í­ì£¼í•˜ê³  ì‹¶ë‹¤ ì™œ ì‹œí—˜ ëë‚˜ì„œ ê¸°ë¶„ ì¢‹ì•„ì„œ ã…‹ã…‹ ë†€ëŸ¬ ê°€ì ì–´ë”” ê°ˆê¹Œ ë†€ì´ê³µì› ê°€ì ì¢‹ì•„ ì´ë²ˆ ì£¼ë§ ì½œ\",\n",
    "    \"ê°•ë„ ë†’ì€ ìš´ë™í–ˆì–´ ë­ í–ˆì–´ í¬ë¡œìŠ¤í• ì‹œì‘í–ˆì–´ í˜ë“¤ì§€ ì•Šì•„ ì²˜ìŒì—ëŠ” ì£½ì„ ê²ƒ ê°™ì•˜ì–´ ã…‹ã…‹ ëŒ€ë‹¨í•˜ë‹¤ ë‚˜ë„ í•´ë³¼ê¹Œ\",\n",
    "    \"ì „ìŸì´ë‹¤ ë­ ì˜¤ëŠ˜ ì„¸ì¼ì¸ë° ì‚¬ëŒì´ ë„ˆë¬´ ë§ì•„ ë­ ì‚¬ë ¤ê³  ìš´ë™í™” ë‚˜ì´í‚¤ ì„¸ì¼ì´ë˜ ë‚˜ë„ ê°€ê³  ì‹¶ë‹¤ ë¹¨ë¦¬ ì™€ ìë¦¬ ë§¡ì•„ë†¨ì–´\",\n",
    "    \"ëˆ ëºê¸¸ ë»”í–ˆì–´ ë­ ë¬´ìŠ¨ ì¼ì´ì•¼ ì•„ë‹ˆ ê²Œì„ì—ì„œ ì‚¬ê¸° ë‹¹í•  ë»” ìš´ì´ ì¢‹ì•˜ë„¤ ë§ì•„ ì¡°ì‹¬í•´ì•¼ì§€ ë‹¤ìŒë¶€í„° í™•ì¸í•˜ê³  ê±°ë˜í•˜ì\",\n",
    "]\n",
    "\n",
    "print(f'ê²½ê³„ ì¼€ì´ìŠ¤: {len(BOUNDARY_CASES)}ê°œ ìƒì„±')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 2-3: ì „ì²´ í•©ì„±ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ â•â•â•\n",
    "print('ğŸš€ í•©ì„±ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘!\\n')\n",
    "\n",
    "src1 = load_smilestyle(max_n=400)\n",
    "src2 = load_kakao(max_n=300)\n",
    "src3 = load_unsmile(max_n=200)\n",
    "src4 = load_nsmc(max_n=100)\n",
    "src5 = BOUNDARY_CASES\n",
    "\n",
    "all_normal = list(set(src1 + src2 + src3 + src4 + src5))\n",
    "random.shuffle(all_normal)\n",
    "\n",
    "# ê¸¸ì´ í•„í„° (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²ƒ ì œê±°)\n",
    "all_normal = [c for c in all_normal if 20 < len(str(c)) < 500]\n",
    "\n",
    "print(f'\\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:')\n",
    "print(f'  SmileStyle: {len(src1)}ê°œ')\n",
    "print(f'  KakaoChat:  {len(src2)}ê°œ')\n",
    "print(f'  UnSmile:    {len(src3)}ê°œ')\n",
    "print(f'  NSMC:       {len(src4)}ê°œ')\n",
    "print(f'  ê²½ê³„ì¼€ì´ìŠ¤: {len(src5)}ê°œ')\n",
    "print(f'  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
    "print(f'  ì¤‘ë³µì œê±° í›„ ì´: {len(all_normal)}ê°œ')\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "normal_df = pd.DataFrame({\n",
    "    'idx': [f'synth_{i:04d}' for i in range(len(all_normal))],\n",
    "    'class': 'ì¼ë°˜ ëŒ€í™”',\n",
    "    'conversation': all_normal,\n",
    "    'label': 4\n",
    "})\n",
    "\n",
    "print(f'\\nê¸¸ì´ í†µê³„: mean={normal_df[\"conversation\"].str.len().mean():.0f}ì')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 2-4: í•™ìŠµ ë°ì´í„° í†µí•© â•â•â•\n",
    "train_df['label'] = train_df['class'].map(CLASS_MAP)\n",
    "normal_df['conv_len'] = normal_df['conversation'].str.len()\n",
    "\n",
    "train_full = pd.concat([train_df, normal_df], ignore_index=True)\n",
    "\n",
    "print('í•©ì„±ë°ì´í„° ì¶”ê°€ í›„ ì „ì²´ í•™ìŠµ ë°ì´í„°:')\n",
    "print(train_full['class'].value_counts())\n",
    "print(f'\\nì´: {len(train_full)}ê°œ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ”§ STEP 3: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "\n",
    "**[ë°°ìš´ ê²ƒ í™œìš©]** Ex06 í…ìŠ¤íŠ¸ ì •ê·œí™”, Ex07 í˜•íƒœì†Œ ë¶„ì„ vs ì„œë¸Œì›Œë“œ í† í°í™”"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 3-1: ì „ì²˜ë¦¬ í•¨ìˆ˜ â•â•â•\n",
    "def preprocess(text):\n",
    "    \"\"\"ì¤„ë°”ê¿ˆâ†’ê³µë°±, ì—°ì†ê³µë°± ì œê±°, strip\"\"\"\n",
    "    if pd.isna(text): return ''\n",
    "    text = str(text).replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_full['text'] = train_full['conversation'].apply(preprocess)\n",
    "test_df['text'] = test_df['conversation'].apply(preprocess)\n",
    "\n",
    "print('ì „ì²˜ë¦¬ ì™„ë£Œ')\n",
    "print(f'  ì „: {train_full[\"conversation\"].iloc[0][:80]}...')\n",
    "print(f'  í›„: {train_full[\"text\"].iloc[0][:80]}...')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ§  STEP 4: ëª¨ë¸ & í† í¬ë‚˜ì´ì € + Dataset ì •ì˜\n",
    "\n",
    "**[ëª¨ë¸ ì„ ì • ê·¼ê±°] KcELECTRA**\n",
    "1. í•œêµ­ì–´ ëŒ“ê¸€/êµ¬ì–´ì²´ ì‚¬ì „í•™ìŠµ â†’ DKTC ë„ë©”ì¸ê³¼ ìœ ì‚¬\n",
    "2. ELECTRA RTD ë°©ì‹: ëª¨ë“  í† í°ì„ í•™ìŠµì— í™œìš© â†’ ì†Œê·œëª¨ ë°ì´í„°ì— íš¨ìœ¨ì \n",
    "3. BERT MLM ëŒ€ë¹„ ë™ì¼ ê³„ì‚°ëŸ‰ì—ì„œ ì„±ëŠ¥ ìš°ìœ„ (Clark et al., 2020)\n",
    "\n",
    "**[ë°°ìš´ ê²ƒ í™œìš©]** Ex09 Transformer êµ¬í˜„ â†’ ì‚¬ì „í•™ìŠµ ëª¨ë¸ Fine-tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 4-1: í† í¬ë‚˜ì´ì € ë¡œë“œ & í† í° ê¸¸ì´ ë¶„ì„ â•â•â•\n",
    "MODEL_NAME = 'beomi/KcELECTRA-base-v2022'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# MAX_LEN ê²°ì • ê·¼ê±°\n",
    "token_lengths = [len(tokenizer.tokenize(t)) for t in train_full['text']]\n",
    "print(f'í† í° ê¸¸ì´ ë¶„í¬:')\n",
    "print(f'  í‰ê· : {np.mean(token_lengths):.0f}')\n",
    "print(f'  90%ile: {np.percentile(token_lengths, 90):.0f}')\n",
    "print(f'  95%ile: {np.percentile(token_lengths, 95):.0f}')\n",
    "print(f'  â†’ MAX_LEN={MAX_LEN}ìœ¼ë¡œ 95%+ ì»¤ë²„')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 4-2: Dataset í´ë˜ìŠ¤ ì •ì˜ â•â•â•\n",
    "# [Ex06 í™œìš©] NSMCDatasetê³¼ ë™ì¼ êµ¬ì¡°\n",
    "\n",
    "class DKTCDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]), max_length=self.max_len,\n",
    "            padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze() for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "print('âœ… DKTCDataset ì •ì˜ ì™„ë£Œ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 4-3: Loss í•¨ìˆ˜ ì •ì˜ â•â•â•\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss (Lin et al., 2017)\n",
    "    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)\n",
    "    â†’ ì‰¬ìš´ ìƒ˜í”Œì˜ lossë¥¼ ì¤„ì´ê³ , ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = (1 - pt) ** self.gamma * ce\n",
    "        if self.alpha is not None:\n",
    "            loss = self.alpha[targets] * loss\n",
    "        return loss.mean()\n",
    "\n",
    "def rdrop_loss(logits1, logits2, labels, criterion, alpha=0.7):\n",
    "    \"\"\"\n",
    "    R-Drop (Liang et al., 2021)\n",
    "    ê°™ì€ ì…ë ¥ 2ë²ˆ forward â†’ ë‘ ì¶œë ¥ì˜ KL divergence ìµœì†Œí™”\n",
    "    â†’ ë“œë¡­ì•„ì›ƒ ëœë¤ì„±ì„ í™œìš©í•œ ì •ê·œí™”\n",
    "    \"\"\"\n",
    "    ce = (criterion(logits1, labels) + criterion(logits2, labels)) / 2\n",
    "    p1 = F.log_softmax(logits1, dim=-1)\n",
    "    p2 = F.log_softmax(logits2, dim=-1)\n",
    "    q1 = F.softmax(logits1, dim=-1)\n",
    "    q2 = F.softmax(logits2, dim=-1)\n",
    "    kl = (F.kl_div(p1, q2, reduction='batchmean') +\n",
    "          F.kl_div(p2, q1, reduction='batchmean')) / 2\n",
    "    return ce + alpha * kl\n",
    "\n",
    "print('âœ… FocalLoss, R-Drop ì •ì˜ ì™„ë£Œ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ”¥ STEP 5: í•™ìŠµ + Ablation Study\n",
    "\n",
    "**[ë°°ìš´ ê²ƒ í™œìš©]** Ex03 Ablation Study ë°©ë²•ë¡ \n",
    "\n",
    "| ì‹¤í—˜ | ì„¤ì • | ë¹„êµ í¬ì¸íŠ¸ |\n",
    "|---|---|---|\n",
    "| Exp1 | CE Loss (baseline) | ê¸°ì¤€ì  |\n",
    "| Exp2 | + Focal Loss | í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° íš¨ê³¼ |\n",
    "| Exp3 | + Focal + R-Drop | ê³¼ì í•© ë°©ì§€ íš¨ê³¼ |\n",
    "| Exp4 | í•©ì„± 500ê°œ ì¶•ì†Œ | ë°ì´í„° ì–‘ì˜ ì˜í–¥ |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 5-1: ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜ â•â•â•\n",
    "\n",
    "def run_experiment(\n",
    "    name, train_data, model_name=MODEL_NAME,\n",
    "    epochs=5, lr=2e-5,\n",
    "    use_focal=False, use_rdrop=False, rdrop_alpha=0.7\n",
    "):\n",
    "    set_seed(42)\n",
    "    print(f'\\n{\"â”€\"*60}')\n",
    "    print(f'ğŸ§ª {name}')\n",
    "    print(f'   Train: {len(train_data)}, Focal: {use_focal}, R-Drop: {use_rdrop}')\n",
    "    print(f'{\"â”€\"*60}')\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    texts = train_data['text'].values\n",
    "    labels = train_data['label'].values\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    tr_loader = DataLoader(\n",
    "        DKTCDataset(X_tr, y_tr, tok, MAX_LEN),\n",
    "        batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    va_loader = DataLoader(\n",
    "        DKTCDataset(X_val, y_val, tok, MAX_LEN),\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Loss\n",
    "    if use_focal:\n",
    "        counts = np.bincount(labels, minlength=NUM_CLASSES).astype(float)\n",
    "        w = 1.0 / np.maximum(counts, 1)\n",
    "        w = w / w.sum() * NUM_CLASSES\n",
    "        criterion = FocalLoss(alpha=torch.FloatTensor(w).to(device), gamma=2.0)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    total_steps = len(tr_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(total_steps*0.1),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    history = {'train_loss':[], 'val_f1':[]}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tr_loader:\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            lbl = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_rdrop:\n",
    "                o1 = model(input_ids=ids, attention_mask=mask)\n",
    "                o2 = model(input_ids=ids, attention_mask=mask)\n",
    "                loss = rdrop_loss(o1.logits, o2.logits, lbl, criterion, rdrop_alpha)\n",
    "            else:\n",
    "                o = model(input_ids=ids, attention_mask=mask)\n",
    "                loss = criterion(o.logits, lbl)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        preds, lbls = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in va_loader:\n",
    "                ids = batch['input_ids'].to(device)\n",
    "                mask = batch['attention_mask'].to(device)\n",
    "                lbl = batch['labels'].to(device)\n",
    "                o = model(input_ids=ids, attention_mask=mask)\n",
    "                preds.extend(torch.argmax(o.logits, dim=-1).cpu().numpy())\n",
    "                lbls.extend(lbl.cpu().numpy())\n",
    "\n",
    "        f1 = f1_score(lbls, preds, average='macro')\n",
    "        tl = total_loss / len(tr_loader)\n",
    "        history['train_loss'].append(tl)\n",
    "        history['val_f1'].append(f1)\n",
    "\n",
    "        star = ''\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), f'best_{name}.pt')\n",
    "            best_preds, best_lbls = preds, lbls\n",
    "            star = ' â­'\n",
    "\n",
    "        print(f'   Ep {epoch+1}/{epochs} | Loss: {tl:.4f} | Val F1: {f1:.4f}{star}')\n",
    "\n",
    "    # ìµœì¢… ë¦¬í¬íŠ¸\n",
    "    model.load_state_dict(torch.load(f'best_{name}.pt'))\n",
    "    print(f'\\n   ğŸ† Best Val F1: {best_f1:.4f}')\n",
    "    print(classification_report(best_lbls, best_preds, target_names=CLASS_NAMES))\n",
    "\n",
    "    return {\n",
    "        'name': name, 'val_f1': best_f1,\n",
    "        'history': history,\n",
    "        'model': model, 'tokenizer': tok,\n",
    "        'preds': best_preds, 'labels': best_lbls\n",
    "    }\n",
    "\n",
    "print('âœ… run_experiment í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 5-2: Exp1 - Baseline (CE Loss) â•â•â•\n",
    "r1 = run_experiment('Exp1_CE_baseline', train_full, use_focal=False, use_rdrop=False)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 5-3: Exp2 - Focal Loss â•â•â•\n",
    "r2 = run_experiment('Exp2_FocalLoss', train_full, use_focal=True, use_rdrop=False)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 5-4: Exp3 - Focal Loss + R-Drop â•â•â•\n",
    "r3 = run_experiment('Exp3_Focal_RDrop', train_full, use_focal=True, use_rdrop=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 5-5: Exp4 - í•©ì„±ë°ì´í„° 500ê°œë¡œ ì¶•ì†Œ â•â•â•\n",
    "n500 = min(500, len(normal_df))\n",
    "normal_500 = normal_df.sample(n=n500, random_state=42)\n",
    "train_500 = pd.concat([\n",
    "    train_df[['text','label']],\n",
    "    normal_500[['text','label']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# text ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì •\n",
    "if 'text' not in train_df.columns:\n",
    "    train_df['text'] = train_df['conversation'].apply(preprocess)\n",
    "if 'text' not in normal_500.columns:\n",
    "    normal_500['text'] = normal_500['conversation'].apply(preprocess)\n",
    "\n",
    "train_500 = pd.concat([\n",
    "    train_df[['text','label']],\n",
    "    normal_500[['text','label']]\n",
    "], ignore_index=True)\n",
    "\n",
    "r4 = run_experiment('Exp4_normal500', train_500, use_focal=True, use_rdrop=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ“ˆ STEP 6: Ablation ê²°ê³¼ ì‹œê°í™”"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 6-1: Ablation ê²°ê³¼ ìš”ì•½ í…Œì´ë¸” â•â•â•\n",
    "results = [r1, r2, r3, r4]\n",
    "baseline_f1 = r1['val_f1']\n",
    "\n",
    "print('=' * 60)\n",
    "print('ğŸ“‹ Ablation Study ê²°ê³¼ ìš”ì•½')\n",
    "print('=' * 60)\n",
    "print(f'{\"ì‹¤í—˜\":<25} {\"Val F1\":>8} {\"ë³€í™”\":>8}')\n",
    "print('â”€' * 45)\n",
    "for r in results:\n",
    "    diff = r['val_f1'] - baseline_f1\n",
    "    d = f'+{diff:.4f}' if diff > 0 else (f'{diff:.4f}' if diff != 0 else '  base')\n",
    "    print(f'{r[\"name\"]:<25} {r[\"val_f1\"]:>8.4f} {d:>8}')\n",
    "\n",
    "best = max(results, key=lambda x: x['val_f1'])\n",
    "print(f'\\nğŸ† Best: {best[\"name\"]} (F1: {best[\"val_f1\"]:.4f})')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 6-2: í•™ìŠµ ê³¡ì„  ë¹„êµ ê·¸ë˜í”„ â•â•â•\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "for i, r in enumerate(results):\n",
    "    axes[0].plot(r['history']['train_loss'], f'-o', color=colors[i], label=r['name'], markersize=4)\n",
    "    axes[1].plot(r['history']['val_f1'], f'-o', color=colors[i], label=r['name'], markersize=4)\n",
    "\n",
    "axes[0].set_title('Train Loss', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Validation F1 Score (Macro)', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ablation_curves.png', dpi=150)\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 6-3: Best ëª¨ë¸ Confusion Matrix â•â•â•\n",
    "cm = confusion_matrix(best['labels'], best['preds'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title(f'Confusion Matrix - {best[\"name\"]} (F1: {best[\"val_f1\"]:.4f})', fontsize=13)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ¯ STEP 7: í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ & ì œì¶œ íŒŒì¼ ìƒì„±"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 7-1: Best ëª¨ë¸ë¡œ Test ì˜ˆì¸¡ â•â•â•\n",
    "best_model = best['model']\n",
    "best_tok = best['tokenizer']\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    DKTCDataset(test_df['text'].values, tokenizer=best_tok, max_len=MAX_LEN),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "best_model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        o = best_model(input_ids=ids, attention_mask=mask)\n",
    "        test_preds.extend(torch.argmax(o.logits, dim=-1).cpu().numpy())\n",
    "\n",
    "print('ì˜ˆì¸¡ ë¶„í¬:')\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    print(f'  {name}: {test_preds.count(i)}ê°œ')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 7-2: submission.csv ìƒì„± â•â•â•\n",
    "submission_df = pd.read_csv('submission.csv')\n",
    "submission_df['class'] = test_preds\n",
    "submission_df.to_csv('submission_final.csv', index=False)\n",
    "\n",
    "print('âœ… submission_final.csv ì €ì¥ ì™„ë£Œ!')\n",
    "print(submission_df.head(10))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 7-3: ì œì¶œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Colab) â•â•â•\n",
    "from google.colab import files\n",
    "files.download('submission_final.csv')\n",
    "print('ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! Kaggleì— ì œì¶œí•˜ì„¸ìš”.')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ“ STEP 8: í”„ë¡œì íŠ¸ ì •ë¦¬ & ë°œí‘œ í¬ì¸íŠ¸"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# â•â•â• 8-1: ìµœì¢… ì •ë¦¬ â•â•â•\n",
    "print(''\n",
    "'''\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           DKTC í”„ë¡œì íŠ¸ ìµœì¢… ì •ë¦¬                        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                          â•‘\n",
    "â•‘  [ë°°ìš´ ê²ƒ í™œìš©]                                          â•‘\n",
    "â•‘  âœ“ EDA & ì „ì²˜ë¦¬: Ex06 í…ìŠ¤íŠ¸ ë¶„ì„                        â•‘\n",
    "â•‘  âœ“ í† í°í™”: Ex07 SentencePiece â†’ ELECTRA WordPiece       â•‘\n",
    "â•‘  âœ“ ëª¨ë¸: Ex09 Transformer â†’ Fine-tuning                 â•‘\n",
    "â•‘  âœ“ Ablation: Ex03 ì‹¤í—˜ ë¹„êµ ë°©ë²•ë¡                        â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•‘  [ìµœì‹  ë…¼ë¬¸ ê¸°ë²•]                                        â•‘\n",
    "â•‘  âœ“ KcELECTRA: í•œêµ­ì–´ êµ¬ì–´ì²´ íŠ¹í™” (Clark 2020)           â•‘\n",
    "â•‘  âœ“ Focal Loss: ë¶ˆê· í˜• í•´ê²° (Lin 2017)                   â•‘\n",
    "â•‘  âœ“ R-Drop: ê³¼ì í•© ë°©ì§€ (Liang 2021)                     â•‘\n",
    "â•‘  âœ“ í•©ì„±ë°ì´í„°: 5ê°œ ì†ŒìŠ¤ í†µí•© (AugGPT ì˜ê°)              â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•‘  [íƒ‘ë‹¤ìš´ ì ‘ê·¼ë²•]                                         â•‘\n",
    "â•‘  ë¨¼ì € ëŒë ¤ë³´ê³  â†’ ë¬¸ì œ ë°œê²¬ â†’ ìµœì‹  ê¸°ë²•ìœ¼ë¡œ í•´ê²°          â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "''')\n",
    "\n",
    "# Ablation ê²°ê³¼ ì¬ì¶œë ¥\n",
    "print('\\nğŸ“‹ Ablation Study ìµœì¢… ê²°ê³¼:')\n",
    "for r in results:\n",
    "    diff = r['val_f1'] - baseline_f1\n",
    "    d = f'+{diff:.4f}' if diff > 0 else (f'{diff:.4f}' if diff != 0 else '  base')\n",
    "    print(f'  {r[\"name\"]:<25} F1: {r[\"val_f1\"]:.4f} ({d})')\n",
    "\n",
    "print(f'\\nğŸ† Best: {best[\"name\"]} â†’ F1: {best[\"val_f1\"]:.4f}')\n",
    "print('\\nğŸ‰ í”„ë¡œì íŠ¸ ì™„ë£Œ!')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
